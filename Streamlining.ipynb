{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import xapian\n",
    "import swifter\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load lookup dataset -- Takes about 2-3 minutes, big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "devsetPath = 'data/devset.json'\n",
    "datapath = 'data/wiki-pages-text/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = set(stopwords.words('english')).union(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    return ' '.join(word for word in set(string.split()) if word not in cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEtag(string, keywordMode=True):\n",
    "    string = unicodes(string)\n",
    "    if keywordMode:\n",
    "        return [ent.text for ent in nlp(string).ents]\n",
    "    else:\n",
    "        return [(ent.text, ent.label_) for ent in nlp(string).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(string):\n",
    "    return set([word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(string)) if pos[0] == 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def slashescape(err):\n",
    "    '''\n",
    "    codecs error handler. err is UnicodeDecode instance. return\n",
    "    a tuple with a replacement for the unencodable part of the input\n",
    "    and a position where encoding should continue\n",
    "    '''\n",
    "    thebyte = err.object[err.start:err.end]\n",
    "    repl = u'\\\\x' + hex(ord(thebyte))[2:]\n",
    "    return (repl, err.end)\n",
    "\n",
    "codecs.register_error('slashescape', slashescape)\n",
    "\n",
    "def unicodes(string):\n",
    "    nfd_string = unicodedata.normalize(\"NFKD\", string)\n",
    "    return nfd_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPropN(string):\n",
    "    doc = nlp(string)\n",
    "    return (' '.join([token.text for token in doc if token.pos_ == 'PROPN'])).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPATH = 'index/indexRN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = xapian.Database(DBPATH)\n",
    "queryparser = xapian.QueryParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryparser.set_stemmer(xapian.Stem(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryparser.add_prefix('keywords', 'K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryparser.add_prefix('about', 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "queryparser.set_stemming_strategy(xapian.QueryParser.STEM_SOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(claim, pagesize=10, show_query=False):\n",
    "    \n",
    "    print('CLAIM:', claim, '\\n') if show_query == True else None\n",
    "    \n",
    "    enquire = xapian.Enquire(database)\n",
    "    keywords = nouns(claim)\n",
    "    about = getPropN(claim)\n",
    "    \n",
    "    '''\n",
    "    Variations:\n",
    "    keywords = NEtag(claim)\n",
    "    keywords = nouns(claim)\n",
    "    keywords = set(get_subject_topic(claim)).union(NEtag(claim))\n",
    "    '''\n",
    "    \n",
    "    query = 'keywords:\"{}\" about:\"{}\" {}'.format(\" AND \".join(keywords), about, claim)\n",
    "    \n",
    "    '''\n",
    "    Variations:\n",
    "    query = 'keywords:\"{}\" {}'.format(\" AND \".join(keywords), claim)\n",
    "    query = 'keywords:{}'.format(\" keywords:\".join(keywords))\n",
    "    query = ' + '.join(clean(claim).split())\n",
    "    query = claim\n",
    "    '''\n",
    "    \n",
    "    # print('NAMED ENTITIES:', NEtag(claim), '\\n') if show_query == True else None\n",
    "    print('FINAL QUERY:', query, '\\n') if show_query == True else None\n",
    "    \n",
    "    # Parse the query and process it on the Database\n",
    "    query = queryparser.parse_query(query)\n",
    "    \n",
    "    print('PARSED QUERY:', query, '\\n') if show_query == True else None\n",
    "    enquire.set_query(query)\n",
    "    matches = enquire.get_mset(0, pagesize)\n",
    "\n",
    "    query_results = []\n",
    "    for match in matches:\n",
    "        result = dict(\n",
    "            claim = claim,\n",
    "            lookedupDoc = match.document.get_data(),\n",
    "            rank = match.rank + 1,\n",
    "            percentage = match.percent,\n",
    "            weight = match.weight,            \n",
    "        )\n",
    "        query_results.append(result)\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageText(pageTitle):\n",
    "    return list(finalDF[finalDF['pageTitle'] == pageTitle]['data'].values)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty Work - Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAIM: Veeru Devgan only works in Hollywood. \n",
      "\n",
      "FINAL QUERY: keywords:\"Devgan AND Hollywood AND Veeru\" about:\"veeru devgan hollywood\" Veeru Devgan only works in Hollywood. \n",
      "\n",
      "PARSED QUERY: Query(((Kdevgan@1 PHRASE 5 Kand@2 PHRASE 5 Khollywood@3 PHRASE 5 Kand@4 PHRASE 5 Kveeru@5) OR (Bveeru@6 PHRASE 3 Bdevgan@7 PHRASE 3 Bhollywood@8) OR (veeru@9 OR devgan@10 OR Zonli@11 OR Zwork@12 OR Zin@13 OR hollywood@14))) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[b'Hindustan_Ki_Kasam_-LRB-1999_film-RRB-', 28, '32.07'],\n",
       " [b'Devgan', 28, '31.95'],\n",
       " [b'Anil_Devgan', 28, '31.45'],\n",
       " [b'Veeru_Devgan', 27, '30.54'],\n",
       " [b'Dil_Kya_Kare', 26, '29.47']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Veeru Devgan only works in Hollywood.'\n",
    "[[item['lookedupDoc'], item['percentage'], '{:.2f}'.format(item['weight'])] for item in search(query,show_query=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation based on devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = pd.read_json(devsetPath, orient='index', encoding='ISO-8859-1')\n",
    "devset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c0429b3d24f0990899a71718aeb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=5001, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "devset['evidence'] = devset['claim'].swifter.apply(lambda x: [[item['lookedupDoc'], 0] for item in search(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset.set_index(('index'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset['evidence'] = devset['evidence'].apply(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset.to_json('data/mydevset10_NE_About_5.json', orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to make a global dictionary which has everey title-sentence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = pd.read_pickle('data/lookupDataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF['pageTitle'] = finalDF['pageTitle'].apply(unicodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalDictionary = pd.Series(finalDF.data.values,index=finalDF.pageTitle).to_dict()\n",
    "del finalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = pd.read_json(devsetPath, orient='index', encoding='ISO-8859-1')\n",
    "devset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstack the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([(tup.index, tup.claim, d, tup.label) for tup in devset.itertuples() for d in tup.evidence], columns=['index', 'claim', 'evidence', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the **NOT ENOUGH INFO** to the unstacked dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = pd.concat([df, devset[devset['label']=='NOT ENOUGH INFO']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractText(pageTitle):\n",
    "    if len(pageTitle) != 0:\n",
    "        pageTitle = [unicodes(pageTitle[0]), pageTitle[1]]\n",
    "        return globalDictionary[pageTitle[0]][pageTitle[1]]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "devset['evidence_text'] = devset['evidence'].apply(lambda x: extractText(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup from index based on claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_results = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307034976a6e457e9c5e13d0c2dfa809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=7369, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "devset['lookup_evidence_title'] = devset['claim'].swifter.apply(lambda x: [item['lookedupDoc'] for item in search(x, pagesize=top_N_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-08e72913acae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decode' is not defined"
     ]
    }
   ],
   "source": [
    "devset['evidence'] = devset['evidence'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
