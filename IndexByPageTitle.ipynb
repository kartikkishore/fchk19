{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datapath = 'data/wiki-pages-text/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "filesInDataPath = sorted([datapath + fileName for fileName in os.listdir(datapath)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sPacy`** - Helper Functions\n",
    "https://spacy.io/usage/spacy-101#annotations-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = set(stopwords.words('english')).union(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodes(string):\n",
    "    nfd_string = unicodedata.normalize(\"NFD\", string)\n",
    "    nfd = nfd_string.encode('WINDOWS-1252', 'ignore')\n",
    "    return nfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    return ' '.join(word for word in set(string.split()) if word not in cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEtag(string, keywordMode=True):\n",
    "    if keywordMode:\n",
    "        return [ent.text for ent in nlp(string).ents]\n",
    "    else:\n",
    "        return [(ent.text, ent.label_) for ent in nlp(string).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(string):\n",
    "    return set([word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(string)) if pos[0] == 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def shardToDF(shardPath):\n",
    "    tempArray = []\n",
    "    with open(shardPath, 'r') as openedFile:\n",
    "        for line in openedFile:\n",
    "            pageTitle, sentenceNo, pageText = line.split(' ', 2)\n",
    "            try:\n",
    "                sentenceNo = int(sentenceNo)\n",
    "                pageText = pageText.replace('-LRB- ','(')\n",
    "                pageText = pageText.replace(' -RRB-',')')\n",
    "                pageText = pageText.replace('-LSB- ','[')\n",
    "                pageText = pageText.replace(' -RSB-',']')\n",
    "            except Exception:\n",
    "                pass\n",
    "            tempArray.append([pageTitle, sentenceNo, pageText])\n",
    "        tempDF = pd.DataFrame.from_records(tempArray, columns=['pageTitle','sentenceNo' ,'pageText'])\n",
    "        return tempDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create information DF for all shards"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "finalDF = pd.DataFrame()\n",
    "with tqdm(total=len(filesInDataPath)) as pbar:\n",
    "    for shardPath in filesInDataPath:\n",
    "        finalDF = pd.concat([finalDF, shardToDF(shardPath)])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating based on Page_Tiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "concatenateFunction = lambda x: ' '.join(x)\n",
    "aggregation_functions = {'pageText': concatenateFunction}\n",
    "newDF = finalDF.groupby(finalDF['pageTitle']).aggregate(aggregation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precomputation: Find Keywords for each line, will take long"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newDF['keywords'] = newDF['pageText'].swifter.apply(lambda x: NEtag(x))\n",
    "newDF.to_pickle('data/nerDataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _`dump`_\n",
    "contains the aggregated dataframe in the form of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.read_pickle('data/nerDataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = newDF.to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAPIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xapian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = 'index/indexRN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xapian build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_db = xapian.WritableDatabase(dbpath, xapian.DB_CREATE_OR_OPEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set termgenerator for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = xapian.TermGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.set_stemmer(xapian.Stem('en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5396106/5396106 [17:32:05<00:00, 85.48it/s]      \n"
     ]
    }
   ],
   "source": [
    "noKeywords = []\n",
    "count = 0\n",
    "with tqdm(total=len(dump)) as pbar:\n",
    "    for pageTitle, pageText, keywords in dump:\n",
    "        count += 1\n",
    "        if len(pageTitle) > 240:\n",
    "            pageTitle = pageTitle[:150]\n",
    "        terInfo = pageTitle.split('_')\n",
    "        indexCounter = u\"Q\" + str(count)\n",
    "\n",
    "        '''Set the data that we want to store'''\n",
    "        xapianDoc = xapian.Document()\n",
    "        xapianDoc.set_data(unicodes(pageTitle))\n",
    "        \n",
    "        # Named Entities are present in keywords + Adding nouns\n",
    "        keywords = set(keywords).union(nouns(pageText))\n",
    "        \n",
    "        # Adding dimension\n",
    "        [xapianDoc.add_term(b\"K\" + unicodes(keyword.lower())) for keyword in keywords if len(keyword) < 150]\n",
    "        [xapianDoc.add_term(b\"B\" + unicodes(keyword.lower())) for keyword in terInfo if len(keyword) < 150]\n",
    "        index.set_document(xapianDoc)\n",
    "        \n",
    "        '''Indexing Based on'''\n",
    "        index.index_text(unicodes(pageText))\n",
    "        index.increase_termpos()\n",
    "        [index.index_text(unicodes(keyword), 1, \"B\") for keyword in set(terInfo)]\n",
    "        [index.index_text(unicodes(keyword), 1, \"K\") for keyword in set(keywords)]\n",
    "            \n",
    "\n",
    "        x_db.replace_document(indexCounter, xapianDoc)\n",
    "        pbar.update(1)\n",
    "    x_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID = d563065e-3d4f-47ec-8f5d-3f07e2e8e8fe\r\n",
      "number of documents = 5396106\r\n",
      "average document length = 291.765\r\n",
      "document length lower bound = 3\r\n",
      "document length upper bound = 71217\r\n",
      "highest document id ever used = 5396106\r\n",
      "has positional information = true\r\n",
      "revision = 540\r\n",
      "currently open for writing = false\r\n"
     ]
    }
   ],
   "source": [
    "!xapian-delve $dbpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
